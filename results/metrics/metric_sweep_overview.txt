decision_tree: Macro-F1 curves show moderate gap indicating balanced bias-variance, with test performance struggling to exceed random-baseline macro-F1. The model benefits markedly from additional training data (Δ=0.238). Inductive bias: High-variance, low-bias learner that memorizes training data unless pruned. The gap narrows as training size increases, indicating improved regularization.
naive_bayes: Macro-F1 curves show moderate gap indicating balanced bias-variance, with test performance struggling to exceed random-baseline macro-F1. The model degrades as the train fraction grows, hinting at instability (Δ=-0.079). Inductive bias: High-bias generative model with strong independence assumptions, generally low variance. The gap narrows as training size increases, indicating improved regularization.
logistic_regression: Macro-F1 curves show moderate gap indicating balanced bias-variance, with test performance solid but improvable macro-F1. The model benefits markedly from additional training data (Δ=0.122). Inductive bias: Linear decision boundaries with L2/L1 regularization to control variance. The gap narrows as training size increases, indicating improved regularization.
knn: Macro-F1 curves show pronounced train-test gap signalling high variance, with test performance solid but improvable macro-F1. The model benefits markedly from additional training data (Δ=0.201). Inductive bias: Instance-based learner where k controls bias-variance; sensitive to neighborhood size. The gap narrows as training size increases, indicating improved regularization.
random_forest: Macro-F1 curves show pronounced train-test gap signalling high variance, with test performance solid but improvable macro-F1. The model benefits markedly from additional training data (Δ=0.257). Inductive bias: Ensemble of decorrelated trees that reduces variance via averaging. The gap narrows as training size increases, indicating improved regularization.
